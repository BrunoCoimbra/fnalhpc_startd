#!/bin/bash

# Terminal utils
BL="\033[1;34m"
YL="\033[1;33m"
WHT="\033[1;97m"
CY="\033[1;36m"
MGT="\033[1;35m"
GR="\033[1;32m"
RD="\033[1;31m"
NO_COLOR="\033[0m"

# Launch a full stack CMS glidein on a set of THETA nodes
# Gathering parameters for the request
THETA_USER=macosta
NODE_CNT=1
QUEUE=debug-cache-quad
TIME="60"
JOB_ID=${RANDOM}
ALLOCATION=HEPCloud-FNAL
verbose='false'

while getopts 'u:n:q:t:v:j:f:h' flag; do
  case "${flag}" in
    u) THETA_USER="${OPTARG}" ;;
    n) NODE_CNT="${OPTARG}" ;;
    q) QUEUE="${OPTARG}" ;;
    t) TIME="${OPTARG}" ;;
    j) JOB_ID="${OPTARG}" ;;
    f) SCHEDD_FILE="${OPTARG}" ;;
    a) ALLOCATION="${OPTARG}" ;;
    v) verbose='true' ;;
    h) echo "Usage: $0"
       echo "Optional: -u [<THETA user>] -n [<Node count>] -q [<THETA queue>] -t [<Node time>] -a [<THETA allocation]>"
       echo "Required: -f [<SCHEDD job queue file]" 1>&2 ; exit 1
       ;;
  esac
done

if [ -n ${SCHEDD_FILE} ]
then
  echo -e "${RD}# ERROR: A required parameter '-f [<SCHEDD job queue file]' was not detected"
  echo -e "${RD}# I need a lumberjack-exported job_queue.log to work properly"
  echo -e "# For insructions about the experimental export process, go to: https://github.com/HEPCloud/fnalhpc_startd/tree/master/lumberjack"
exit 1
fi 

SLOT_PREFIX="cobalt-cms-${JOB_ID}"
GLIDEIN_NAME="cobalt-cms-${JOB_ID}@theta.alcf.anl.gov"
THETA_BASE_DIR="/projects/${ALLOCATION}/job_area/${SLOT_PREFIX}"

echo "====== Submitting a Lumberjack cobalt job from ${HOSTNAME}"
echo "Local Linux user: $(whoami)"
LNS=$(qstat -u ${THETA_USER} | wc -l)
if [ $LNS -gt 2 ]
then
  echo "WARN: There are COBALT jobs in the queue"
  qstat -u ${THETA_USER}
fi

echo ""
echo ====== Creating base directory on shared storage
mkdir ${THETA_BASE_DIR}
echo Using directory $THETA_BASE_DIR

# Set up local Condor installation
echo ""
echo ====== Writing base files
echo Copy the "skeleton" folder as-is
cp -dR skeleton/* $THETA_BASE_DIR/
cp -dR minicondor_lumberjack_start.sh $THETA_BASE_DIR/minicondor_lumberjack_start.sh
echo Put the job_queue.log file in place
cp -dR ${SCHEDD_FILE} $THETA_BASE_DIR/local_dir/lib/condor/spool/job_queue.log
echo Generate and make sure permissions are well set for the pool_password
openssl rand -hex 32 > $THETA_BASE_DIR/secrets/pool_password
chmod 400 $THETA_BASE_DIR/secrets/pool_password
echo "Add custom names to our daemons"
cat >$THETA_BASE_DIR/condor/config.d/97-personal2.conf <<EOF
# Run-time settings for this personal condor setup
#new startd name
MASTER_NAME = master_${GLIDEIN_NAME}
COLLECTOR_NAME = coll_${GLIDEIN_NAME} 
SCHEDD_NAME = sched_${GLIDEIN_NAME}
STARTD_NAME = startd_${GLIDEIN_NAME}
EOF

echo ""
echo "====== Analyzing job_queue.log for directories needed by my jobs"
echo ""
SANDBOX_DIR=${THETA_BASE_DIR}/sandbox
ORIG_SCHEDD_HOST=$(grep -wR 'GlobalJobId' job_queue.log | awk '{print $4}' | tr -d \" | awk -F# '{print $1}' | sort -u)
echo -e "${YL}Input job_queue.log indicates my source Schedd host is: ${ORIG_SCHEDD_HOST}${NO_COLOR}"
WORKDIR_LIST=$(grep -R Iwd job_queue.log | awk '{print $4}' | sort -u | tr -d \")

# This for loop is not needed, we are assuming there's a single, common IWD for all exported jobs
# But it might come helpful if we figure out how to do this cleanly
for dir in $WORKDIR_LIST 
do
#   echo "Original IWD: $dir"
   ORIG_IWD=${dir}
   NEWDIR="${PWD}/${SLOT_PREFIX}/sandbox"
   mkdir -p $SANDBOX_DIR
#   echo "HPC-minicondor IWD: ${SANDBOX_DIR}"
done


echo -e "${MGT}"
echo -e "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
echo -e "# Lumberjack does not yet support file transfer                          "
echo -e "# Make sure the following files from the original Schedd are available to"
echo -e "# this HPC-minicondor job by placing them into:                          "
echo -e "# $SANDBOX_DIR"
echo -e "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
echo -e ""
echo "Original IWD: $dir"
echo "HPC-minicondor IWD: ${SANDBOX_DIR}"
echo ""
# Files we will transfer/list: Cmd + TransferInput
XFERS=$(grep -wR 'TransferInput' job_queue.log | awk '{print $4}' | sort -u | tr -d \")
IFS=',' read -ra XFER_LIST <<< "$XFERS"

ORIG_SCHEDD_CMD=$(grep -wR 'Cmd' job_queue.log | awk '{print $4}' | sort -u | tr -d \")
ORIG_CMD=$(grep -wR 'Cmd' job_queue.log | awk '{print $4}' | sort -u | awk -F/ '{print $NF}' | tr -d \")
FILE_LIST="${ORIG_SCHEDD_CMD}"
#  echo -e "scp root@${ORIG_SCHEDD_HOST}:$ORIG_SCHEDD_CMD ${NEWDIR}/${ORIG_CMD}"
for file in "${XFER_LIST[@]}"
do
    NEWLOC="${SANDBOX_DIR}/$file"
    FILE_LIST="${FILE_LIST} ${ORIG_IWD}/$file"
#    echo "scp root@${ORIG_SCHEDD_HOST}:${ORIG_IWD}/$file ${NEWLOC}"
done
echo -e "# Tips:"
echo "To pull the files from the local(HPC) login node, first make sure that there is networking connectivity between the login node and the remote schedd via SSH (port 22), files will be copied via 'scp'. If both networking and authentication via ssh are possible from the login node to the remote Schedd, this script will **automatically** atemmpt to run the following scp command and pull the necessary files:" | fold | awk '{ print "\t" $0 }'
echo ""
echo "    > scp root@${ORIG_SCHEDD_HOST}:'${FILE_LIST}' ${SANDBOX_DIR}"
echo ""
echo "To push the files from the remote schedd machine, if there is only one-way connectivity, as is the case of FNAL (our machines can not be accessed from offsite). You'll need to login to the Schedd and 'scp' the files over to the login node, which by default has inbound ssh connectivity. The caveat here is that we still need someone (Maria) to do this by hand with an MFA token that lives in her phone. If that is your case, please login to your Schedd machine ${SCHEDD_ORIG_HOST} and run the following instruction:" | fold | awk '{ print "\t" $0 }'
echo ""
echo "    > scp ${FILE_LIST} $(whoami)@theta.alcf.anl.gov:${SANDBOX_DIR}"
echo ""

# Create UserLog, Out and Err directories"
OUT=$(grep -wR 'Out' job_queue.log | awk '{print $4}' | awk -F/ '{print $1}' | tr -d \" | sort -u)
ERR=$(grep -wR 'Err' job_queue.log | awk '{print $4}' | awk -F/ '{print $1}' | tr -d \" | sort -u)
LOG=$(grep -wR 'UserLog' job_queue.log | awk '{print $4}' | awk -F/ '{print $1}' | tr -d \" | sort -u)
[ -z "$OUT" ] && echo "" || mkdir ${SANDBOX_DIR}/$OUT
[ -z "$ERR" ] && echo "" || mkdir ${SANDBOX_DIR}/$ERR
[ -z "$LOG" ] && echo "" || mkdir ${SANDBOX_DIR}/condor_log

## Verify if we have a krb ticket, if so, attempt to SCP the files while we're at it
kticket=`klist 2> /dev/null | grep FNAL.GOV`
if [ -n "$kticket" ]; then
  echo -e "${GR}Found a Kerberos ticket, I will attempt to copy files from ${ORIG_SCHEDD_HOST}"
  echo -e "scp root@${ORIG_SCHEDD_HOST}:$ORIG_SCHEDD_CMD ${NEWDIR}/${ORIG_CMD}"
  echo -e "scp root@${ORIG_SCHEDD_HOST}:'${FILE_LIST}' ${SANDBOX_DIR}"
else
    echo "No Kerberos ticket found! The following files are expected, please make sure they are present before the HPC job starts"
    echo 'See "Tips:" above'
    echo "- ${SANDBOX_DIR}/${ORIG_CMD}"
    for file in "${XFER_LIST[@]}"
    do
      NEWLOC="${SANDBOX_DIR}/$file"
      echo "- $NEWLOC"
    done
fi
echo ""
echo -e "Done, verify that the file tree looks good"
echo ""
ls -ltrha ${SANDBOX_DIR}

exit 0

# write theta files
echo ====== Writing files for job submission
cat > ${THETA_BASE_DIR}/job.cobalt <<EOF
#!/bin/bash

# number of nodes
#COBALT -n ${NODE_CNT}
# wall time request, this is 30min
#COBALT -t ${TIME}
# one of the two debug queues, only difference is KNL cache settings
#COBALT -q ${QUEUE}
# project, this should work for the ALCC allocation
#COBALT -A ${ALLOCATION}

export MY_JOBID=${JOB_ID}
# MPI launcher, 1 node, 1 MPI rank per node
# basically starts mycommand on both nodes in the batch job
EOF

# Next
# Submitting the "head" job with: Master, Collector, Negotiator, Schedd and Startds (in case I'm running on a single node)
echo "aprun -n 1 -N 1 -d 1 -j 1 --cc none --env SLOT_PREFIX=${SLOT_PREFIX} --env COBALT_NODEID=1 ./minicondor_lumberjack_start.sh -r CentralManager &" >> ${THETA_BASE_DIR}/job.cobalt
# Submitting "worker" jobs with: Master, Startd
for ((i=2;i<=${NODE_CNT};i++)); do
    echo "aprun -n 1 -N 1 -d 1 -j 1 --cc none --env SLOT_PREFIX=${SLOT_PREFIX} --env COBALT_NODEID=${i} ./minicondor_lumberjack_start.sh -r Execute &" >> ${THETA_BASE_DIR}/job.cobalt
    echo "sleep 1" >> ${THETA_BASE_DIR}/job.cobalt
done

echo "wait" >> ${THETA_BASE_DIR}/job.cobalt

chmod a+rx ${THETA_BASE_DIR}/job.cobalt

echo INFO: Files written

echo INFO: Submitting COBALT job 

# Submit cobalt job
#cd ${THETA_BASE_DIR}; qsub -A ${ALLOCATION} -q ${QUEUE} -t ${TIME} -n ${NODE_CNT} --jobname=${SLOT_PREFIX} --attr ssds=required job.cobalt
echo INFO: Done, check your job and files at ${THETA_BASE_DIR}
